Earthquake Data Collection Project

A system for automatically collecting, processing, and analyzing earthquake data from the USGS API.

## Quick Start

### Via Docker (recommended)

1. **Initialize Airflow:**
```bash
export AIRFLOW_UID=$(id -u) # for Linux/Mac (optional)
docker-compose up airflow-init
```

2. **Start all services:**
```bash
docker-compose up -d
```

3. **Access web interfaces:**
- Airflow: http://localhost:8081 (login: `airflow`, password: `airflow`)
- Kafka UI: http://localhost:8080

4. **Activate DAGs:**
- Open Airflow UI
- Enable DAGs: `job1_continuous_ingestion`, `job2_hourly_cleaning`, `job3_daily_analytics`

ðŸ“– **Detailed instructions:** See [DOCKER_SETUP.md](DOCKER_SETUP.md)

## Architecture

- **Job 1**: Collect data from USGS API â†’ Kafka (every minute)
- **Job 2**: Read from Kafka â†’ Cleanup â†’ SQLite (every hour)
- **Job 3**: Data analytics â†’ daily_summary (every day)

## Components

- **Apache Kafka** - message queue
- **Apache Airflow** - task orchestration
- **PostgreSQL** - Airflow metadata
- **SQLite** - storage of processed data

## Project Structure

``
. â”œâ”€â”€ docker-compose.yaml # Docker configuration
â”œâ”€â”€ Dockerfile.airflow # Airflow image
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ DOCKER_SETUP.md # Detailed instructions
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ airflow.cfg # Airflow configuration
â”‚ â””â”€â”€ dags/ # DAG files
â”œâ”€â”€ src/ # Source code
â””â”€â”€ data/ # SQLite database
```

## Useful commands

```bash
# View logs
docker-compose logs -f

# Stop
docker-compose stop

# Complete cleanup
docker-compose down -v
```

## Requirements

- Docker 20.10+
- Docker Compose 1.29+
- 4GB RAM minimum
- 10GB of free space